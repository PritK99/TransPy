# Transformers-from-Scratch

## Table of Contents

- [Project](#Transformers-from-Scratch)
  - [Table of Contents](#table-of-contents)
  - [About The Project](#about-the-project)
  - [Demo](#demo)
  - [File Structure](#file-structure)
  - [Contributors](#contributors)
  - [References](#references)
  - [License](#license)
  

## About

Building a custom Transformer model from scratch in PyTorch for English-to-Spanish translation.

<img src = "./assets/Transformer-architecture.png" alt="The Architecture of Transformer Model">

## File Structure
```
ğŸ‘¨â€ğŸ’»Transformers-from-Scratch
 â”£ ğŸ“‚assets                            // Contains all the reference gifs, images
 â”£ ğŸ“‚documentation                     // Contains documentation and my notes on transformers
 â”ƒ â”£ ğŸ“„README.md
 â”£ ğŸ“„model.py                          // Code for Transformer Architecture
 â”£ ğŸ“„train.py                          // Tokenizers
 â”£ ğŸ“„dataset.py                        // Datasets  
 â”£ ğŸ“„README.md
``` 

## References
* <a href="https://www.youtube.com/watch?v=ISNdQcPhsts&t=2729s">YouTube Video</a> by Umar Jamil on developing transformers from scratch.
* <a href="https://arxiv.org/abs/1706.03762">Link</a> to ```Attention is all you need``` paper explaining transformer architecture
* <a href="https://huggingface.co/datasets/opus_books">opus_books</a> dataset by huggingface
* Amirhossein Kazemnejad's Blog on <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Positional Encodings</a>
 
## License
[MIT License](https://opensource.org/licenses/MIT)